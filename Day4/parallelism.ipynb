{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism\n",
    "\n",
    "Computers nowadays all have multiple cores that can run code in parallel. Many codes and algorithms in astronomy can be catagorized as \"embarrassingly parallel\", meaning that they can easily be split up into multiple tasks that each have little or no dependency on each other. An example of this is running the same image processing steps on multipel files, or computing the likelihood of a bunch of different sets of models. \n",
    "\n",
    "Parallelism adds extra complexity to the code, making it harder to debug and maintain. Thus, it is important to consider the relative gain of putting in parallelism versus the extra effort in developing and maintaining that code. For this reason, we also recommend that you keep parallelization code as simple as possible. For many tasks, even the most simple parallelism is sufficient. \n",
    "\n",
    "We will discuss a few options to do parallelism in Python in order of increasing complexity:\n",
    "\n",
    "  1. BLAS/LAPACK\n",
    "  3. Thread/process pools\n",
    "\n",
    "TODO: monitor your CPU/RAM utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAS/LAPACK/ATLAS/MKL\n",
    "\n",
    "There are packages that automatically parallelize linear algebra routines like matrix dot product or singular value decomposition. BLAS/LAPACK  are the algorithms, and ATLAS, OpenBLAS, or MKL are implementations of them. If you have an Intel processor, you will be using MKL. They are quite useful for when you don't want to write your own parallelization code but want to speed up matrix routines. It requires numpy to be configured for BLAS/MKL properly. Generally if you install numpy with Anaconda, this happens automatically, so we recommend simply doing it this way. These speedups are turned on by default when configured.\n",
    "\n",
    "However, if you wish to do your own parallelism, make sure to turn this off. Having multiple layers of parallelism will often slow down your code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "blas_mkl_info:\n    libraries = ['mkl_rt', 'pthread']\n    library_dirs = ['/home/jwang/anaconda3/lib']\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n    include_dirs = ['/home/jwang/anaconda3/include']\nblas_opt_info:\n    libraries = ['mkl_rt', 'pthread']\n    library_dirs = ['/home/jwang/anaconda3/lib']\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n    include_dirs = ['/home/jwang/anaconda3/include']\nlapack_mkl_info:\n    libraries = ['mkl_rt', 'pthread']\n    library_dirs = ['/home/jwang/anaconda3/lib']\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n    include_dirs = ['/home/jwang/anaconda3/include']\nlapack_opt_info:\n    libraries = ['mkl_rt', 'pthread']\n    library_dirs = ['/home/jwang/anaconda3/lib']\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n    include_dirs = ['/home/jwang/anaconda3/include']\n"
    }
   ],
   "source": [
    "# Check we have BLAS/LAPACK. Notice they are from the MKL library\n",
    "import numpy as np\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4 cores 4.90791590011213\n1 core 12.856466499972157\n"
    }
   ],
   "source": [
    "import mkl\n",
    "import timeit\n",
    "\n",
    "mat = np.random.random((1000, 1500)) # 1000x1500 matrix\n",
    "\n",
    "# Use 4 cores to parallelize matrix operations\n",
    "mkl.set_num_threads(4)\n",
    "print(\"4 cores\", timeit.timeit(\"mat.dot(mat.T)\", setup=\"from __main__ import mat\", number=100))\n",
    "\n",
    "# Disable MKL\n",
    "mkl.set_num_threads(1)\n",
    "print(\"1 core\", timeit.timeit(\"mat.dot(mat.T)\", setup=\"from __main__ import mat\", number=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Multiple Python Instances\n",
    "\n",
    "This might sound very unsophisticated, but it is actually a good choice for many instances. As you will learn before, Python parallelism is not the best, and if tasks are completely independent of each other, running each task as a separate Python process and saving the result as a file is a perfectly reasonable (and simple) option. This is great for batch processes such as bulk processing a bunch of files with some data reduction code.\n",
    "\n",
    "There are many options to do this: bash script, GNU Parallel, or, as we will focus on today, a master python script. We will focus on using a python script to launch a bunch of python processes because all capabilities of shell scripting (e.g., calling bash commands with the sys module) can be done in Python, and often with much better readability. We will use python to launch a bunch of python processes.\n",
    "\n",
    "We create a function with an argument `index` that tells each proces what chunk of the task to run. We then create a bunch of processes, give them their chunks, and call `start()` to run them. Afterwards, our master process uses `join()` to wait for each process to finish. It is important to always call `join()` at the end to ensure all processes have finished running! If a process has finished immediately, `join()` will immediately return; if a process has not finished, our master process will sleep until the process it is waiting on has finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Process 0 complete\nProcess 1 complete\nProcess 2 complete\nProcess 3 complete\nProcess 4 complete\nProcess 5 complete\nProcess 6 complete\nProcess 7 complete\nProcess 8 complete\nProcess 9 complete\n"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def matrix_loop(mat, index):\n",
    "    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n",
    "    index_start = mat.shape[0] // 10 * index\n",
    "    index_end = mat.shape[0] // 10 * (index + 1)\n",
    "    val  = mat[index_start:index_end].dot(mat.T)\n",
    "    print(\"Process {0} complete\".format(index))\n",
    "    # could save value to a shared variable or save to a file to be used later\n",
    "\n",
    "process_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    p = multiprocessing.Process(target=matrix_loop, args=(mat, i))\n",
    "    process_list.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in process_list:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pools\n",
    "\n",
    "Manually creating threads requires a bunch of upkeep code, which is unnecessary if you are just running over a giant loop. In the spirit of keeping parallelism simple, use high-level parallelization API provided by your programming language whenever possible! It will save you time and effort (Trust me). For dividing up tasks with a for loop, use Python process `Pools`. Essentially, you can give\n",
    "any number of tasks to a process `Pool` and the processes in the pool will loop through and do each one per your instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Created job 0\nCreated job 1\nCreated job 2\nCreated job 3\nCreated job 4\nCreated job 5\nCreated job 6\nCreated job 7\nCreated job 8\nCreated job 9\nProcess 0 complete\nResult 0 (100, 1000)\nProcess 1 complete\nResult 1 (100, 1000)\nProcess 2 complete\nResult 2 (100, 1000)\nProcess 3 complete\nResult 3 (100, 1000)\nProcess 4 complete\nResult 4 (100, 1000)\nProcess 5 complete\nResult 5 (100, 1000)\nProcess 6 complete\nResult 6 (100, 1000)\nProcess 7 complete\nResult 7 (100, 1000)\nProcess 8 complete\nResult 8 (100, 1000)\nProcess 9 complete\nResult 9 (100, 1000)\n"
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=4) # creae a pool with 4 worker processes\n",
    "\n",
    "def matrix_loop(mat, index):\n",
    "    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n",
    "    index_start = mat.shape[0] // 10 * index\n",
    "    index_end = mat.shape[0] // 10 * (index + 1)\n",
    "    val  = mat[index_start:index_end].dot(mat.T)\n",
    "    print(\"Process {0} complete\".format(index))\n",
    "    \n",
    "    return val # let's return the data this time\n",
    "\n",
    "pool_jobs = []\n",
    "for i in range(10):\n",
    "    job = pool.apply_async(matrix_loop, (mat, i))\n",
    "    pool_jobs.append(job)\n",
    "    print(\"Created job {0}\".format(i))\n",
    "\n",
    "for i, job in enumerate(pool_jobs):\n",
    "    result = job.get() \n",
    "    print(\"Result {0}\".format(i), result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics in Parallelization\n",
    "\n",
    "## Threads vs Processes\n",
    "\n",
    "## Shared Variables\n",
    "\n",
    "Do you really need it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}